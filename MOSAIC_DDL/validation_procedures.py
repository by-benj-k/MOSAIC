"""
validation_procedures.py

This module contains some functions helpful for validating the seeds and documents generated by MOSAIC_DDL.

Author: Benjamin Koch
Date: July 2025
"""

# Imports
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt
from rapidfuzz import fuzz
from typing import Union
import config_framework
import seaborn as sns
import pandas as pd
import json


def compute_allowed_attributes_per_text_type() -> dict[str, dict[str, set[str]]]:
    """
    Creates a mapping from domain names to text types occuring in the respective domain. The text types are again mapped to the attributes which are allowed to occur.

    Returns:
        dict[str, dict[str, set[str]]]
    """

    # Load configuration file
    config_tree = ET.parse(config_framework.CONFIG)
    config_root = config_tree.getroot()

    # Collect all attributes of each domain respectively to later replace the all quantifier
    all_attributes = {}
    for domain in config_root.find("domains").findall("domain"):
        attributes = set()
        for domain_attribute in domain.findall("domainAttribute"):
            attributes.add(domain_attribute.get("id"))
        for entity in domain.find("entities").findall("entity"):
            for entity_attribute in entity.findall("entityAttribute"):
                attributes.add(entity_attribute.get("id"))
        all_attributes[domain.get("id")] = attributes

    # Fetch occurring attributes for each text type of all domains
    allowed_attributes = {}
    for domain in config_root.find("domains").findall("domain"):
        attributes_for_text_type = {}
        for texttype in domain.find("texttypes").findall("texttype"):
            attributes_for_text_type[texttype.get("id")] = set((texttype.find(
                "occurringAttributes").get("value")).split(","))

            # Replace all quantifier
            if attributes_for_text_type[texttype.get("id")] == {"all"}:
                attributes_for_text_type[texttype.get(
                    "id")] = all_attributes[domain.get("id")]

        allowed_attributes[domain.get("id")] = attributes_for_text_type

    return allowed_attributes


def compute_allowed_attributes_and_entities_per_text_type() -> dict[str, dict[str, set[str]]]:
    """
    Creates a mapping from domain names to text types occuring in the respective domain. The text types are again mapped to the attributes and entities which are allowed to occur.

    Returns:
        dict[str, dict[str, set[str]]]
    """

    # Load configuration file
    config_tree = ET.parse(config_framework.CONFIG)
    config_root = config_tree.getroot()

    # Collect all attributes of each domain respectively to later replace the all quantifier
    all_attributes = {}
    for domain in config_root.find("domains").findall("domain"):
        attributes = set()
        for domain_attribute in domain.findall("domainAttribute"):
            attributes.add(domain_attribute.get("id"))
        for entity in domain.find("entities").findall("entity"):
            for entity_attribute in entity.findall("entityAttribute"):
                attributes.add(entity_attribute.get("id"))
        all_attributes[domain.get("id")] = attributes

    # Fetch occurring attributes for each text type of all domains; create set of entities belonging to the entity attributes and add them as well
    allowed_attributes_and_entities = {}
    for domain in config_root.find("domains").findall("domain"):
        attributes_for_text_type = {}
        for texttype in domain.find("texttypes").findall("texttype"):
            attributes_for_text_type[texttype.get("id")] = set((texttype.find(
                "occurringAttributes").get("value")).split(","))

            # Replace all quantifier
            if attributes_for_text_type[texttype.get("id")] == {"all"}:
                attributes_for_text_type[texttype.get(
                    "id")] = all_attributes[domain.get("id")]

            # Compute entities of occurring attributes
            entities = {".".join(attribute.split(".")[
                                 :2]) for attribute in attributes_for_text_type[texttype.get("id")] if attribute.count(".") == 2}

            # Add entities to allowed attribute and entities
            attributes_for_text_type[texttype.get("id")].update(entities)

        allowed_attributes_and_entities[domain.get(
            "id")] = attributes_for_text_type

    return allowed_attributes_and_entities


def compute_keys_of_seed(inputDict: dict) -> set[str]:
    """
    Computes the set of all keys of a potentially nested dictionary structure.

    Parameters:
        inputDict (dict): The dictionary of which we want to fetch all keys.

    Returns:
        set[str]: The set containing all keys.
    """

    # Storage for keys
    keys = set()

    if isinstance(inputDict, dict):
        for key, value in inputDict.items():
            if key == "domain" or key == "text_type":
                continue

            if isinstance(value, list):
                list_has_dict = False
                for subDict in value:
                    if isinstance(subDict, dict):
                        keys.update(compute_keys_of_seed(subDict))
                        list_has_dict = True

                if not list_has_dict:
                    keys.add(key)
            else:
                keys.add(key)

    return keys


def compute_keys_including_entities_of_seed(inputDict: dict) -> set[str]:
    """
    Computes the set of all keys of a potentially nested dictionary structure, including the entity list starters.

    Parameters:
        inputDict (dict): The dictionary of which we want to fetch all keys.

    Returns:
        set[str]: The set containing all keys.
    """

    # Storage for keys
    keys = set()

    if isinstance(inputDict, dict):
        for key, value in inputDict.items():
            if key == "domain" or key == "text_type":
                continue

            if isinstance(value, list):
                keys.add(key)
                for subDict in value:
                    if isinstance(subDict, dict):
                        keys.update(compute_keys_of_seed(subDict))
            else:
                keys.add(key)

    return keys


def make_hashable(value: Union[str, int, list[str]]) -> Union[str, int, tuple]:
    """
    Converts an unhashable list into a tuple which is hashable.

    Parameters:
        value (Union[str, int, list[str]]): The input object.

    Returns:
        Union[str, int, tuple]: Either returns element unchanged or converts list to tuple.
    """

    if isinstance(value, list):
        return tuple(value)
    return value


def compute_keys_and_values_of_seed(inputDict: dict) -> set[tuple[str, Union[str, int, list[str]]]]:
    """
    Computes the set of all keys and values of a potentially nested dictionary structure.

    Parameters:
        inputDict (dict): The dictionary of which we want to fetch all keys.

    Returns:
        set[tuple[str, Union[str, int, list[str]]]]: The set containing all keys.
    """

    # Storage for keys
    keys = set()

    if isinstance(inputDict, dict):
        for key, value in inputDict.items():
            if key == "domain" or key == "text_type":
                continue

            if isinstance(value, list):
                list_has_dict = False
                for subDict in value:
                    if isinstance(subDict, dict):
                        keys.update(compute_keys_and_values_of_seed(subDict))
                        list_has_dict = True

                if not list_has_dict:
                    keys.add((key, make_hashable(value)))
            else:
                keys.add((key, make_hashable(value)))

    return keys


def validate_attribute_frequency() -> None:
    """
    This function validates, whether the blanked seeds generated adhere to the frequencies specified for the individual attributes. It is important to note, that this function is only expected to return the approximate frequency specified for each attribute if no relations are defined, since the co-occurrence could affect the occurrence of the attributes.

    Notes: Small parts of this function were developed with the help of ChatGPT.
    """

    # Fetch allowed attributes
    allowed_attributes = compute_allowed_attributes_per_text_type()

    # Storage for all attribute statistics (dictionary mapping attribute name to list [#blank seeds attribute occurs in, #blank seed where the attribute is allowed to occur])
    attribute_statistics = {}

    # Traverse blanked seeds
    with open(config_framework.BLANK_SEEDS, "r", encoding='utf-8') as blank_seeds:
        for blank_seed in blank_seeds:
            loaded_blank_seed = json.loads(blank_seed)
            occurring_attributes = compute_keys_of_seed(loaded_blank_seed)
            text_type = loaded_blank_seed["text_type"]

            # Update number of blank seeds where attribute is allowed to occur in
            for attribute in allowed_attributes[loaded_blank_seed["domain"]][text_type]:
                if attribute not in attribute_statistics:
                    attribute_statistics[attribute] = [0, 1]
                else:
                    attribute_statistics[attribute][1] += 1

            # Update number of blank seeds where attribute actually occurs in
            for attribute in occurring_attributes:
                if attribute in attribute_statistics.keys():
                    attribute_statistics[attribute][0] += 1

    # Compute fractions of computed statistics
    attribute_statistics_fractions = {}
    for key, value in attribute_statistics.items():
        attribute_statistics_fractions[key] = value[0] / value[1]

    # Fetch actual frequency probabilities for all attributes
    config_tree = ET.parse(config_framework.CONFIG)
    config_root = config_tree.getroot()
    frequencies = {}
    for domain in config_root.find("domains").findall("domain"):
        for domain_attribute in domain.findall("domainAttribute"):
            frequencies[domain_attribute.get(
                "id")] = float(domain_attribute.get("frequency"))
        for entity in domain.find("entities").findall("entity"):
            for entity_attribute in entity.findall("entityAttribute"):
                frequencies[entity_attribute.get(
                    "id")] = float(entity_attribute.get("frequency"))

    # Create dataframe of results for seaborn
    df = pd.DataFrame({"Attribute": list(attribute_statistics_fractions.keys()), "Computed Frequency": [
                      attribute_statistics_fractions[key] for key in attribute_statistics_fractions], "Target Frequency": [frequencies[key] for key in attribute_statistics_fractions]})

    # Sort data in descending order of computed frequency value
    df = df.sort_values(by=["Target Frequency", "Attribute"],
                        ascending=[False, True])

    # Set theme, set font and size
    sns.set_theme(style="whitegrid", font="Palatino Linotype", font_scale=0.75)
    plt.figure(figsize=(20, 10))

    # Plot bars and markers
    ax = sns.barplot(x="Attribute", y="Computed Frequency",
                     data=df, color="#cfe4ff", saturation=1.0, label="Computed Frequency")
    sns.scatterplot(x="Attribute", y="Target Frequency",
                    data=df, zorder=10, s=120, color="red", edgecolor="black", label="Target Frequency")

    # Rotate x axis labels for better readability
    plt.xticks(rotation=45, ha="right")
    ax.tick_params(axis="x", labelsize=11)
    ax.tick_params(axis="y", labelsize=15)
    plt.ylim(0, 1)

    # Set title and axis names
    plt.title("Computed Frequency vs Target Frequency", fontsize=15)
    plt.ylabel("Frequency", fontsize=15)
    plt.xlabel("Attribute", fontsize=15)
    plt.legend(fontsize=15)

    # Adjust path
    path = config_framework.VALIDATION_EVALUATION_FOLDER
    new_path = path + "validate_attribute_frequency.pdf"

    # Set tight layout and save image
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(new_path, dpi=200)


def validate_attribute_transfer() -> None:
    """
    This function validates whether the information provdided to the document generation model are actually being transferred into the texts. It is important to note that this is at most a heuristic! Even though we use a fuzzy string matcher, it is very likely that the documents generated by the model contain the information in a differently written way and will thus not be recognized by this procedure.

    Notes: Small parts of this function were developed with the help of ChatGPT.
    """

    # Storage for all attribute statistics (dictionary mapping attribute name to list [#documents attribute occurs in, #blank seed where the attribute occurs in])
    attribute_statistics = {}

    # Traverse blanked seed and documents
    with open(config_framework.DOCUMENTS, "r", encoding='utf-8') as documents, open(config_framework.BLANK_SEEDS, "r", encoding='utf-8') as blank_seeds:
        for document, blank_seed in zip(documents, blank_seeds):
            # Load document
            loaded_document = json.loads(document)["document"].lower()

            # Load blank seed
            loaded_blank_seed = json.loads(blank_seed)
            occurring_attributes = compute_keys_and_values_of_seed(
                loaded_blank_seed)

            # Update blank seed count where the attribute occurs in
            for attribute, _ in occurring_attributes:
                if attribute not in attribute_statistics.keys():
                    attribute_statistics[attribute] = [0, 1]
                else:
                    attribute_statistics[attribute][1] += 1

            # Traverse occurring attributes and check for occurrence in documents
            for attribute, value in occurring_attributes:
                if isinstance(value, str) or isinstance(value, int) or isinstance(value, float):
                    if fuzz.partial_ratio(str(value).lower(), loaded_document) >= 95:
                        attribute_statistics[attribute][0] += 1
                elif isinstance(value, tuple):
                    if all(fuzz.partial_ratio(str(v).lower(), loaded_document) >= 95 for v in value):
                        attribute_statistics[attribute][0] += 1

    # Create dataframe of results for seaborn
    df = pd.DataFrame({"Attribute": list(attribute_statistics.keys()), "Frequency in Documents": [
                      attribute_statistics[key][0] for key in attribute_statistics], "Frequency in Blank Seeds": [attribute_statistics[key][1] for key in attribute_statistics]})

    # Normalize values
    maximum = max(df["Frequency in Documents"].max(),
                  df["Frequency in Blank Seeds"].max())
    df["Frequency in Documents"] = df["Frequency in Documents"] / maximum
    df["Frequency in Blank Seeds"] = df["Frequency in Blank Seeds"] / maximum

    # Sort data in descending order of computed frequency value
    df = df.sort_values(by=["Frequency in Blank Seeds",
                        "Attribute"], ascending=[False, True])

    # Set theme, set font and size
    sns.set_theme(style="whitegrid", font="Palatino Linotype", font_scale=0.75)
    plt.figure(figsize=(20, 10))

    # Plot bars and markers
    ax = sns.barplot(x="Attribute", y="Frequency in Documents",
                     data=df, color="#cfe4ff", saturation=1.0, label="Frequency in Documents")
    sns.scatterplot(x="Attribute", y="Frequency in Blank Seeds",
                    data=df, zorder=10, s=120, color="red", edgecolor="black", label="Frequency in Blank Seeds")

    # Rotate x axis labels for better readability
    plt.xticks(rotation=45, ha="right")
    ax.tick_params(axis="x", labelsize=11)
    ax.tick_params(axis="y", labelsize=15)
    plt.ylim(0, 1)
    # plt.yscale("log")

    # Set title and axis names
    plt.title("Frequency in Documents vs Frequency in Blank Seeds", fontsize=15)
    plt.ylabel("Frequency", fontsize=15)
    plt.xlabel("Attribute", fontsize=15)
    plt.legend(fontsize=15)

    # Adjust path
    path = config_framework.VALIDATION_EVALUATION_FOLDER
    new_path = path + "validate_attribute_transfer.pdf"

    # Set tight layout and save image
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(new_path, dpi=200)


def validate_cooccurrence_frequency() -> None:
    """
    This function, whether the seeds generated adhere to the relations defined. It is important to note that the frequencies computed here might still change during document generation.

    Notes: Small parts of this function were developed with the help of ChatGPT.
    """

    # Fetch allowed attributes and entities
    allowed_attributes_and_entities = compute_allowed_attributes_and_entities_per_text_type()

    # Storage for relations
    relations = set()

    # Fetch all relations as pairs
    config_tree = ET.parse(config_framework.CONFIG)
    config_root = config_tree.getroot()

    for domain in config_root.find("domains").findall("domain"):
        for relation in domain.find("relations").findall("relation"):
            if relation.get("type") == "cooccurrence":
                relations.add((relation.find("from").get("ref"),
                              relation.find("to").get("ref")))
            else:
                pass

    # Storage for statistics
    relations_statistics = {}

    # Traverse seeds
    for (from_ref, to_ref) in relations:
        # Initialize storage
        relations_statistics[" <-> ".join([from_ref, to_ref])] = [0, 0]

        with open(config_framework.SEEDS, "r", encoding='utf-8') as seeds:
            for seed in seeds:
                # Load seed
                seed = json.loads(seed)

                # Compute keys (including entitiy list starters) of seed
                seed_keys = compute_keys_including_entities_of_seed(seed)

                # Check if both end points occur in keys
                if from_ref in seed_keys and to_ref in seed_keys:
                    relations_statistics[" <-> ".join(
                        [from_ref, to_ref])][0] += 1
                    relations_statistics[" <-> ".join(
                        [from_ref, to_ref])][1] += 1
                elif from_ref in seed_keys and to_ref not in seed_keys and from_ref in allowed_attributes_and_entities[seed["domain"]][seed["text_type"]] and to_ref in allowed_attributes_and_entities[seed["domain"]][seed["text_type"]]:
                    relations_statistics[" <-> ".join(
                        [from_ref, to_ref])][1] += 1

    # Compute fractions of computed statistics
    relations_statistics_fractions = {}
    for key, value in relations_statistics.items():
        relations_statistics_fractions[key] = value[0] / value[1]

    # Fetch actual frequencies
    frequencies = {}
    for domain in config_root.find("domains").findall("domain"):
        for relation in domain.find("relations").findall("relation"):
            if relation.get("type") == "cooccurrence":
                frequencies[" <-> ".join([relation.find("from").get("ref"), relation.find(
                    "to").get("ref")])] = float(relation.get("probability"))
            else:
                pass

    # Create dataframe of results for seaborn
    df = pd.DataFrame({"Relation": list(relations_statistics_fractions.keys()), "Computed Frequency": [
                      relations_statistics_fractions[key] for key in relations_statistics_fractions], "Target Frequency": [frequencies[key] for key in relations_statistics_fractions]})

    # Sort data in descending order of computed frequency value
    df = df.sort_values(by=["Target Frequency", "Relation"],
                        ascending=[False, True])

    # Set theme, set font and size
    sns.set_theme(style="whitegrid", font="Palatino Linotype", font_scale=0.75)
    plt.figure(figsize=(20, 10))

    # Plot bars and markers
    ax = sns.barplot(x="Relation", y="Computed Frequency",
                     data=df, color="#cfe4ff", saturation=1.0, label="Computed Frequency")
    sns.scatterplot(x="Relation", y="Target Frequency",
                    data=df, zorder=10, s=120, color="red", edgecolor="black", label="Target Frequency")

    # Rotate x axis labels for better readability
    plt.xticks(rotation=45, ha="right")
    ax.tick_params(axis="x", labelsize=15)
    ax.tick_params(axis="y", labelsize=15)
    plt.ylim(0, 1)

    # Set title and axis names
    plt.title("Computed Frequency vs Target Frequency", fontsize=15)
    plt.ylabel("Frequency", fontsize=15)
    plt.xlabel("Relation", fontsize=15)
    plt.legend(fontsize=15)

    # Adjust path
    path = config_framework.VALIDATION_EVALUATION_FOLDER
    new_path = path + "validate_cooccurrence_frequency.pdf"

    # Set tight layout and save image
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(new_path, dpi=200)


def run_validation_procedures() -> None:
    """
    Runs all validation procedures.
    """

    # Attribute frequency validation
    validate_attribute_frequency()

    # Attribute transfer validation
    validate_attribute_transfer()

    # Co-Occurrence frequency validation
    validate_cooccurrence_frequency()
